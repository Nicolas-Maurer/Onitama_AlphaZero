{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eba4a14",
   "metadata": {},
   "source": [
    "#### Source for the architecture\n",
    "\n",
    "https://arxiv.org/pdf/1712.01815.pdf\n",
    "\n",
    "https://www.chessprogramming.org/AlphaZero#:~:text=AlphaZero%20evaluates%20positions%20using%20non,policy)%20and%20a%20position%20evaluation.\n",
    "\n",
    "http://www.talkchess.com/forum3/viewtopic.php?f=2&t=69175&start=93\n",
    "\n",
    "residual block : https://www.chessprogramming.org/Neural_Networks#Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86238f",
   "metadata": {},
   "source": [
    "# Input size : 5 x 5 x 10\n",
    "- board size : 5 x 5 \n",
    "- P1 unique pieces : 2 \n",
    "- P2 unique pieces : 2\n",
    "- P1 cards moves : 2\n",
    "- P2 cards moves : 2\n",
    "- Remaining card\n",
    "- Colour : 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d89f242",
   "metadata": {},
   "source": [
    "# Output size 5 x 5 x 13\n",
    "- board size : 5 x 5 \n",
    "- 13 possibles moves, 1 for each directions in [\"N\", \"NE\", \"E\", \"SE\", \"S\", \"SW\", \"W\", \"NW\"], plus the dragon, crab and tiger differents moves  \n",
    "\n",
    "So a total of 5x5x13 = 325 moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53f9aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f496aef5",
   "metadata": {},
   "source": [
    "# Functionnal NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7f45de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1453349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_block \n",
    "input_block = layers.Input(shape=(5, 5, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8613ed04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolutionnal_layer \n",
    "x = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"linear\")(input_block)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.LeakyReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba4091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 Residual blocks with a skip connection\n",
    "for _ in range(19):\n",
    "    y = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", strides=1, activation=\"linear\")(x)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "    \n",
    "    y = layers.Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", strides=1, activation=\"linear\")(y)\n",
    "    y = layers.BatchNormalization()(y)\n",
    "    y = layers.LeakyReLU()(y)\n",
    "    \n",
    "    x = layers.Add()([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7192582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy_head with a final convolution of 13 filters\n",
    "policy_head = layers.Conv2D(filters=256, kernel_size=(1, 1), padding=\"same\", activation=\"linear\")(x)\n",
    "policy_head = layers.BatchNormalization()(policy_head)\n",
    "policy_head = layers.LeakyReLU()(policy_head)\n",
    "policy_head = layers.Conv2D(filters=13, kernel_size=(1, 1), padding=\"same\", activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72aecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_head\n",
    "value_head = layers.Conv2D(filters=1, kernel_size=(1,1), padding=\"same\", strides=1, activation=\"linear\")(x)\n",
    "value_head = layers.BatchNormalization()(value_head)\n",
    "value_head = layers.LeakyReLU()(value_head)\n",
    "value_head = layers.Flatten()(value_head)\n",
    "value_head = layers.Dense(256, activation=\"linear\")(value_head)\n",
    "value_head = layers.LeakyReLU()(value_head)\n",
    "value_head = layers.BatchNormalization()(value_head)\n",
    "value_head = layers.Dense(1, activation=\"tanh\", name=\"value_head\")(value_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50fce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[input_block], outputs=[policy_head, value_head])\n",
    "model.compile(loss=['categorical_crossentropy','mean_squared_error'], optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0215a1",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Self play some game with a neural network f. \n",
    "Try to minimize the difference between policy and possible policy, and value and Q-value ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
