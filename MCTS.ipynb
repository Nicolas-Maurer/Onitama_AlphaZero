{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://joshvarty.github.io/AlphaZero/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Board.ipynb\n",
    "%run NNet_architecture.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonteCarloTreeSearchNode():\n",
    "    \n",
    "    def __init__(self, model, board, prior, parent=None, parent_action=None, name=None):\n",
    "        self.model = model\n",
    "        self.board = board\n",
    "        self.prior = prior\n",
    "        self.parent = parent\n",
    "        self.name = name\n",
    "        self.parent_action = parent_action\n",
    "        self.children = {}\n",
    "        self._number_of_visits = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "        self._results = defaultdict(int)\n",
    "        self._results[0] = 0\n",
    "        self._results[1] = 0\n",
    "        self._results[-1] = 0\n",
    "\n",
    "            \n",
    "    def expand(self, possible_policy):\n",
    "        \"\"\" Expand the node with all children with a positive probability, the policy is obtained by the nn\"\"\"\n",
    "                \n",
    "        for i, proba in enumerate(possible_policy):\n",
    "            if proba != 0:\n",
    "                \n",
    "                next_board = self.board.move(i) \n",
    "                self.children[i] = MonteCarloTreeSearchNode(model=model, board=next_board, prior=proba, parent=self)\n",
    "                \n",
    "\n",
    "    def simulate(self, nb_simulation):\n",
    "        \"\"\" Simulate i path\"\"\"\n",
    "        \n",
    "        \n",
    "        if self.children == {}:\n",
    "            \n",
    "            policy, value = self.model.predict(self.board.board_state.reshape((1, 5, 5, 10)))\n",
    "            possible_policy = self.board.get_legal_moves(policy[0]).flatten()\n",
    "            \n",
    "            self.expand(possible_policy)\n",
    "        for _ in range(nb_simulation):\n",
    "                        \n",
    "            node_to_expand = self.max_UCB() # child that maximize UCB\n",
    "\n",
    "            value = node_to_expand.board.get_reward_for_player() \n",
    "\n",
    "            if value is None:\n",
    "                # if the game has not ended we expand \n",
    "\n",
    "                policy, value = node_to_expand.model.predict(node_to_expand.board.board_state.reshape((1, 5, 5, 10)))\n",
    "                possible_policy = node_to_expand.board.get_legal_moves(policy[0]).flatten()\n",
    "                \n",
    "                node_to_expand.expand(possible_policy)\n",
    "            \n",
    "            node_to_expand.backpropagate(value)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def backpropagate(self, value):\n",
    "        self._number_of_visits += 1.\n",
    "        self.value_sum += value\n",
    "        if self.parent:\n",
    "            self.parent.backpropagate(-value)\n",
    "\n",
    "#######################################################################\n",
    "   \n",
    "    def max_UCB(self):\n",
    "        \"\"\"Return the node to expand, the one that maximize UCB\"\"\"\n",
    "        \n",
    "        current_node = self\n",
    "        while current_node.children:\n",
    "            current_node = current_node.best_child()\n",
    "            current_node.max_UCB()\n",
    "\n",
    "        return current_node \n",
    "\n",
    "    def best_child(self, c_param=0.2):\n",
    "        \"\"\"return child that maximize UCB\"\"\"\n",
    "\n",
    "        # C_param is the exploration rate it's supposed to grow slowly with search time\n",
    "        # Mean action_value + C_param * Prior * sqrt(parent visit count) / (1 + visit count)\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "\n",
    "        child_value = [(action, -child.mean_value() + c_param * child.prior * np.sqrt(child.parent._number_of_visits) / (child._number_of_visits + 1))\n",
    "                       for action, child in self.children.items()]\n",
    "    \n",
    "        action, best_child = max(child_value, key = lambda x:x[1])\n",
    "        return self.children[action]\n",
    "    \n",
    "    def mean_value(self):\n",
    "        if self._number_of_visits == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self._number_of_visits\n",
    "\n",
    "#######################################################################\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Debugger pretty print node info\n",
    "        \"\"\"\n",
    "        prior = \"{0:.2f}\".format(self.prior)\n",
    "        return \"{} Prior: {} Count: {} Value: {}\".format(self.board.__str__(), prior, self._number_of_visits, self.mean_value())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_board = Board(deck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = MonteCarloTreeSearchNode(model, new_board, prior=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root.simulate(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root.children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def ucb_score(parent, child):\n",
    "    \"\"\"\n",
    "    The score for an action that would transition between the parent and child.\n",
    "    \"\"\"\n",
    "    prior_score = child.prior * math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score\n",
    "\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, prior, to_play):\n",
    "        self.visit_count = 0\n",
    "        self.to_play = to_play\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.state = None\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def select_action(self, temperature):\n",
    "        \"\"\"\n",
    "        Select action according to the visit count distribution and the temperature.\n",
    "        \"\"\"\n",
    "        visit_counts = np.array([child.visit_count for child in self.children.values()])\n",
    "        actions = [action for action in self.children.keys()]\n",
    "        if temperature == 0:\n",
    "            action = actions[np.argmax(visit_counts)]\n",
    "        elif temperature == float(\"inf\"):\n",
    "            action = np.random.choice(actions)\n",
    "        else:\n",
    "            # See paper appendix Data Generation\n",
    "            visit_count_distribution = visit_counts ** (1 / temperature)\n",
    "            visit_count_distribution = visit_count_distribution / sum(visit_count_distribution)\n",
    "            action = np.random.choice(actions, p=visit_count_distribution)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def select_child(self):\n",
    "        \"\"\"\n",
    "        Select the child with the highest UCB score.\n",
    "        \"\"\"\n",
    "        best_score = -np.inf\n",
    "        best_action = -1\n",
    "        best_child = None\n",
    "\n",
    "        for action, child in self.children.items():\n",
    "            score = ucb_score(self, child)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_action = action\n",
    "                best_child = child\n",
    "\n",
    "        return best_action, best_child\n",
    "\n",
    "    def expand(self, state, to_play, action_probs):\n",
    "        \"\"\"\n",
    "        We expand a node and keep track of the prior policy probability given by neural network\n",
    "        \"\"\"\n",
    "        self.to_play = to_play\n",
    "        self.state = state\n",
    "        for a, prob in enumerate(action_probs):\n",
    "            if prob != 0:\n",
    "                self.children[a] = Node(prior=prob, to_play=self.to_play * -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Debugger pretty print node info\n",
    "        \"\"\"\n",
    "        prior = \"{0:.2f}\".format(self.prior)\n",
    "        return \"{} Prior: {} Count: {} Value: {}\".format(self.state.__str__(), prior, self.visit_count, self.value())\n",
    "\n",
    "\n",
    "class MCTS:\n",
    "\n",
    "\n",
    "    def run(self, model, state, board, to_play, nb_simu):\n",
    "\n",
    "        root = Node(0, to_play)\n",
    "\n",
    "        # EXPAND root\n",
    "        \n",
    "        policy, value = model.predict(state.board.board_state.reshape((1, 5, 5, 10)))\n",
    "        possible_policy = state.board.get_legal_moves(policy[0]).flatten()\n",
    "        \n",
    "        root.expand(state, to_play, possible_policy)\n",
    "\n",
    "        for _ in range(nb_simu):\n",
    "            node = root\n",
    "            search_path = [node]\n",
    "\n",
    "            # SELECT\n",
    "            while node.expanded():\n",
    "                action, node = node.select_child()\n",
    "                search_path.append(node)\n",
    "\n",
    "            parent = search_path[-2]\n",
    "            state = parent.state\n",
    "            # Now we're at a leaf node and we would like to expand\n",
    "            # Players always play from their own perspective\n",
    "            next_state = board.move(action=action)\n",
    "            # Get the board from the perspective of the other player\n",
    "            # The value of the new state from the perspective of the other player\n",
    "            value = next_state.get_reward_for_player()\n",
    "            if value is None:\n",
    "\n",
    "                # If the game has not ended:\n",
    "                # EXPAND\n",
    "                \n",
    "                policy, value = model.predict(next_state.board_state.reshape((1, 5, 5, 10)))\n",
    "                possible_policy = next_state.get_legal_moves(policy[0]).flatten()\n",
    "                \n",
    "                node.expand(next_state, parent.to_play * -1, possible_policy)\n",
    "\n",
    "            self.backpropagate(search_path, value, parent.to_play * -1)\n",
    "            \n",
    "        return root\n",
    "\n",
    "    def backpropagate(self, search_path, value, to_play):\n",
    "        \"\"\"\n",
    "        At the end of a simulation, we propagate the evaluation all the way up the tree\n",
    "        to the root.\n",
    "        \"\"\"\n",
    "        for node in reversed(search_path):\n",
    "\n",
    "            node.value_sum += value if node.to_play == to_play else -value\n",
    "    \n",
    "            node.visit_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = MCTS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a = test.run(model, root, root.board, 1, nb_simu=80)\n",
    "# a.children"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
